{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d933f1-ee11-431a-904c-d6ee7b26d26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb8f410-dc27-4c1d-83ac-1e40e07f9fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773977ab-54cf-48c3-b99f-42848c34b929",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pdfminer.six\n",
    "#pip show pdfminer\n",
    "pip show pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e0bc73-8a9c-4de8-89ef-3b87a349d132",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28ff2d7-04b0-42bf-81c9-6353392f9929",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e53f7a-9a37-4641-b30a-b3932fd28588",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93dec8f-022c-45b1-96bd-aa703e9ca945",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --force-reinstall pdfminer.six==20231228\n",
    "\n",
    "!pip show pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a44f91d-00f6-4c95-8d97-a00821384ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger_eng: <urlopen\n",
      "[nltk_data]     error [Errno 11001] getaddrinfo failed>\n",
      "[nltk_data] Error loading maxent_ne_chunker_tab: <urlopen error [Errno\n",
      "[nltk_data]     11001] getaddrinfo failed>\n",
      "[nltk_data] Error loading words: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Enter the path to your resume (PDF or DOCX):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  Divi_Kaustubh_Abhinand_Resume_Black.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================\n",
      "ðŸ“‹ Resume Summary\n",
      "===============================\n",
      "Name: Divi Kaustubh\n",
      "Email: kaustabhinanddivi@gmail.com\n",
      "Phone: None\n",
      "Education: Vasavi College of Engineering B.Tech in Computer Science (Expected Graduation: 2027) Relevant Coursework: Data Structures & Algorithms, Operating Systems, Database Management Systems\n",
      "Skills (11): Machine Learning, Java, Operating Systems, classification, SQL, Data Structures, MATLAB, C, Artificial Intelligence, scheduling, Python\n",
      "About Me: Motivated and passionate Computer Science student with a strong foundation in programming, problem-solving, and software development. Eager to explore opportunities in AI & ML, software engineering, and database management. Enthusiastic about continuous learning and leveraging technical skills to solve real-world challenges.\n",
      "===============================\n",
      "ATS Score: 63.29/100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import docx2txt\n",
    "from pdfminer.high_level import extract_text\n",
    "#import pdfminer.high_level\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import joblib\n",
    "from joblib import load\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "MODEL_PATH = \"ats_model.pkl\"\n",
    "VECTORIZER_PATH = \"tfidf_vectorizer.pkl\"\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')\n",
    "\n",
    "if os.path.exists(MODEL_PATH) and os.path.exists(VECTORIZER_PATH):\n",
    "    model = joblib.load(MODEL_PATH)\n",
    "    vectorizer = joblib.load(VECTORIZER_PATH)\n",
    "    MODEL_AVAILABLE = True\n",
    "else:\n",
    "    print(\"ATS model not found!\")\n",
    "    MODEL_AVAILABLE = False\n",
    "\n",
    "\n",
    "skills_df = pd.read_excel(\"skills.xlsx\")\n",
    "SKILLS_DB = skills_df.iloc[:, 0].dropna().str.lower().tolist()\n",
    "\n",
    "RESERVED_EDU_WORDS = ['university', 'college', 'institute', 'school', 'academy', 'faculty']\n",
    "\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    txt = docx2txt.process(docx_path)\n",
    "    if txt:\n",
    "        return txt.replace('\\t', ' ')\n",
    "    return \"\"\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path) or \"\"\n",
    "\n",
    "\n",
    "\n",
    "#  Extract Email & Phone\n",
    "\n",
    "def extract_contact_info(text):\n",
    "    email = None\n",
    "    phone = None\n",
    "\n",
    "    # Email\n",
    "    email_match = re.search(r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+', text)\n",
    "    if email_match:\n",
    "        email = email_match.group(0)\n",
    "\n",
    "    # Phone\n",
    "    phone_match = re.search(r'(\\+?\\d{1,3}[\\s\\-]?)?\\(?\\d{2,4}\\)?[\\s\\-]?\\d{3,5}[\\s\\-]?\\d{3,5}', text)\n",
    "    if phone_match:\n",
    "        phone = phone_match.group(0)\n",
    "\n",
    "    return email, phone\n",
    "\n",
    "\n",
    "\n",
    "#  Extract Name \n",
    "\n",
    "def extract_name(text):\n",
    "    lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "    first_line = lines[0]\n",
    "    words = nltk.word_tokenize(first_line)\n",
    "    tagged = nltk.pos_tag(words)\n",
    "\n",
    "    proper_nouns = [word for word, pos in tagged if pos == 'NNP']\n",
    "    if len(proper_nouns) >= 2:\n",
    "        return f\"{proper_nouns[0]} {proper_nouns[1]}\"\n",
    "    elif proper_nouns:\n",
    "        return proper_nouns[0]\n",
    "    return first_line\n",
    "\n",
    "#  Extract Education\n",
    "\n",
    "def extract_education(text):\n",
    "    RESERVED_EDU_WORDS = ['university', 'college', 'institute', 'school', 'academy', 'faculty']\n",
    "    DEGREE_KEYWORDS = [\n",
    "        'b.tech', 'b.e', 'bsc', 'b.s', 'bachelor',\n",
    "        'm.tech', 'm.e', 'msc', 'm.s', 'master',\n",
    "        'mba', 'phd', 'diploma', 'degree', 'b.ed'\n",
    "    ]\n",
    "\n",
    "    lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "    education_entries = set()\n",
    "\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        lower_line = lines[i].lower()\n",
    "\n",
    "        # Start only if we explicitly hit \"Education\" heading\n",
    "        if re.match(r'education', lower_line):\n",
    "            j = i + 1\n",
    "            entry = \"\"\n",
    "\n",
    "            while j < len(lines):\n",
    "                next_line = lines[j].strip()\n",
    "                # Stop if new section begins\n",
    "                if re.search(r'about\\s*me|technical skills|certifications|projects|experience|interests|extracurricular|email|phone', next_line, re.I):\n",
    "                    break\n",
    "                entry += \" \" + next_line\n",
    "                j += 1\n",
    "\n",
    "            education_entries.add(entry.strip())\n",
    "            i = j\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    return education_entries\n",
    "\n",
    "\n",
    "\n",
    "def extract_skills(text):\n",
    "\n",
    "\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    word_tokens = nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "\n",
    "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
    "\n",
    "\n",
    "    filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
    "\n",
    "\n",
    "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
    "\n",
    "\n",
    "    found_skills = set()\n",
    "\n",
    "\n",
    "    for token in filtered_tokens:\n",
    "        if token.lower() in SKILLS_DB:\n",
    "            found_skills.add(token)\n",
    "\n",
    "\n",
    "    for ngram in bigrams_trigrams:\n",
    "        if ngram.lower() in SKILLS_DB:\n",
    "            found_skills.add(ngram)\n",
    "\n",
    "    return found_skills\n",
    "\n",
    "# extract About me\n",
    "\n",
    "def extract_about_me(text):\n",
    "    lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "    about_me = [] \n",
    "\n",
    "   \n",
    "    ABOUT_SECTION_HEADERS = [\n",
    "        r'about\\s*me',\n",
    "        r'career\\s*objective',\n",
    "        r'career\\s*objectives',\n",
    "        r'objective',\n",
    "        r'summary',\n",
    "        r'professional\\s*summary',\n",
    "        r'profile',\n",
    "        r'personal\\s*statement',\n",
    "        r'future\\s*objectives',\n",
    "        r'future\\s*goals'\n",
    "    ]\n",
    "\n",
    "  \n",
    "    SECTION_ENDERS = [\n",
    "        r'education', r'technical\\s*skills', r'certifications',\n",
    "        r'projects', r'experience', r'work\\s*experience',\n",
    "        r'interests', r'extracurricular', r'skills'\n",
    "    ]\n",
    "\n",
    "    about_start_regex = re.compile('|'.join(ABOUT_SECTION_HEADERS), re.I)\n",
    "    \n",
    "   \n",
    "    about_end_regex = re.compile(r'^\\s*(' + '|'.join(SECTION_ENDERS) + r')\\b', re.I)\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        # If this line indicates the start of the About/Objective/Summary section\n",
    "        if about_start_regex.search(line):\n",
    "            j = i + 1\n",
    "            \n",
    "            while j < len(lines):\n",
    "                next_line = lines[j].strip()\n",
    "\n",
    "              \n",
    "                is_header_match = about_end_regex.search(next_line)\n",
    "                is_short_line = len(next_line.split()) <= 5\n",
    "                \n",
    "                # Stop only if it looks like a distinct header\n",
    "                if is_header_match and is_short_line:\n",
    "                    break\n",
    "\n",
    "                about_me.append(next_line)\n",
    "                j += 1\n",
    "            \n",
    "            break # stop after extracting the section\n",
    "\n",
    "    return \" \".join(about_me).strip()\n",
    "\n",
    "    \n",
    "# clean the text (remove punctuation and digits).\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "   \n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    \n",
    "    stop = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop])\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# ATS score prediction\n",
    "def predict_ats_score(text):\n",
    "    if not MODEL_AVAILABLE:\n",
    "        return None\n",
    "    cleaned = clean_text(text)\n",
    "    vector = vectorizer.transform([cleaned])\n",
    "    score = model.predict(vector)[0]\n",
    "    return round(float(score), 2)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"ðŸ“„ Enter the path to your resume (PDF or DOCX):\")\n",
    "    resume_path = input(\"> \").strip()\n",
    "\n",
    "    if resume_path.lower().endswith('.pdf'):\n",
    "        text = extract_text_from_pdf(resume_path)\n",
    "    elif resume_path.lower().endswith('.docx'):\n",
    "        text = extract_text_from_docx(resume_path)\n",
    "    else:\n",
    "        print(\"Unsupported file format. Please use PDF or DOCX.\")\n",
    "        exit()\n",
    "\n",
    "    name = extract_name(text)\n",
    "    email, phone = extract_contact_info(text)\n",
    "    skills = extract_skills(text)\n",
    "    education = extract_education(text)\n",
    "    about_me = extract_about_me(text)\n",
    "    cleaned_text = clean_text(text)\n",
    "\n",
    "    print(\"\\n===============================\")\n",
    "    print(\"ðŸ“‹ Resume Summary\")\n",
    "    print(\"===============================\")\n",
    "    print(f\"Name: {name}\")\n",
    "   \n",
    "    print(f\"Email: {email}\")\n",
    "    print(f\"Phone: {phone}\")\n",
    "    print(f\"Education: {', '.join(education) if education else 'Not found'}\")\n",
    "    print(f\"Skills ({len(skills)}): {', '.join(skills)}\")\n",
    "    print(f\"About Me: {about_me if about_me else 'Not found'}\")\n",
    "    print(\"===============================\")\n",
    "\n",
    "    if MODEL_AVAILABLE:\n",
    "            vector = vectorizer.transform([cleaned_text])\n",
    "            raw_pred = model.predict(vector)[0]\n",
    "\n",
    "        \n",
    "            pred = max(0, min(100, raw_pred))\n",
    "\n",
    "            print(f\"ATS Score: {pred:.2f}/100\")\n",
    "\n",
    "            #print(f\"ATS Score: {pred}/100\")\n",
    "    else:\n",
    "            print(\"ðŸ’¡ ATS model unavailable. Train it first to enable scoring.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cfc4a7-8738-4369-84ae-524f085cdbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bb1be8-2c9d-43fc-b8bd-3b3f999cbe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pdfminer.six\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd332c4-156e-4ad8-a942-7aece7f87481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"master_resumes.jsonl\", lines=True)\n",
    "df.shape        # shows number of rows and columns\n",
    "df.head()       # displays first 5 rows\n",
    "df.columns      # shows available columns\n",
    "print(df.columns)\n",
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89501882-5011-46d0-ac89-52718a75e11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dict object into raw text\n",
    "def flatten_field(field):\n",
    "    \n",
    "    if isinstance(field, dict):\n",
    "        parts = []\n",
    "        for k, v in field.items():\n",
    "            parts.append(flatten_field(v))\n",
    "        return ' '.join(parts)\n",
    "    elif isinstance(field, list):\n",
    "        return ' '.join(flatten_field(item) for item in field)\n",
    "    elif isinstance(field, str):\n",
    "        return field\n",
    "    elif pd.isna(field):\n",
    "        return ''\n",
    "    else:\n",
    "        return str(field)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6613ddbd-9b9d-42b7-a56e-739e70d2e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = (\n",
    "    df[\"personal_info\"].apply(flatten_field) + \" \" +\n",
    "    df[\"experience\"].apply(flatten_field) + \" \" +\n",
    "    df[\"education\"].apply(flatten_field) + \" \" +\n",
    "    df[\"skills\"].apply(flatten_field) + \" \" +\n",
    "    df[\"projects\"].apply(flatten_field) + \" \" +\n",
    "    df[\"certifications\"].apply(flatten_field) + \" \" +\n",
    "    df[\"achievements\"].apply(flatten_field) + \" \" +\n",
    "    df[\"workshops\"].apply(flatten_field) + \" \" +\n",
    "    df[\"publications\"].apply(flatten_field) + \" \" +\n",
    "    df[\"teaching_experience\"].apply(flatten_field) + \" \" +\n",
    "    df[\"internships\"].apply(flatten_field)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d520526-bcf7-48fc-97cc-34938fc2d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"text\"].iloc[0][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b42513-c6ed-4828-89d1-dc031d8bc4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a synthetic set of scores for training purpose. Act as true values\n",
    "def synthetic_score(text):\n",
    "    text = str(text).lower()\n",
    "    score = 0\n",
    "\n",
    "    # core sections\n",
    "    if \"education\" in text or \"university\" in text or \"b.tech\" in text:\n",
    "        score += 10\n",
    "    if \"experience\" in text or \"developer\" in text or \"engineer\" in text:\n",
    "        score += 10\n",
    "    if \"project\" in text:\n",
    "        score += 10\n",
    "    if \"skill\" in text or \"programming\" in text:\n",
    "        score += 10\n",
    "\n",
    "    # tech stack keywords\n",
    "    if any(k in text for k in [\"python\", \"java\", \"c++\", \"sql\", \"machine learning\", \"ai\", \"data\"]):\n",
    "        score += 10\n",
    "\n",
    "    # extras\n",
    "    if \"intern\" in text:\n",
    "        score += 5\n",
    "    if \"certificate\" in text or \"certification\" in text:\n",
    "        score += 5\n",
    "    if \"achievement\" in text or \"award\" in text:\n",
    "        score += 5\n",
    "\n",
    "    # word count (resume length)\n",
    "    wc = len(text.split())\n",
    "    if 200 < wc < 600:\n",
    "        score += 10\n",
    "    elif wc >= 600:\n",
    "        score += 5\n",
    "\n",
    "    # leadership/initiative\n",
    "    if any(k in text for k in [\"lead\", \"manage\", \"developed\", \"designed\", \"built\"]):\n",
    "        score += 10\n",
    "\n",
    "    return min(score, 100)\n",
    "\n",
    "    \n",
    "df[\"score\"] = df[\"text\"].apply(synthetic_score)\n",
    "print(\"Successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653c8274-8b40-4b6c-a477-76f1c630f7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"text\", \"score\"]].head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533882d9-d6fa-4eb3-86a2-bad458679e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(df[\"score\"], bins=10)\n",
    "plt.title(\"Synthetic ATS Score Distribution\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Number of Resumes\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9502b6a-a61e-4b97-a3a7-0fb3816aba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the data\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    # remove punctuation and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # remove stopwords\n",
    "    stop = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop])\n",
    "\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
    "print(\"Successfully cleaned!\")\n",
    "print(df[\"clean_text\"].iloc[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7b18e8-2975-42d4-8115-2c2e391216a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the ATS model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge   \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=3000)\n",
    "X = vectorizer.fit_transform(df[\"clean_text\"])\n",
    "y = df[\"score\"].values \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "model = Ridge(alpha=1.0)   \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "y_pred_clipped = np.clip(y_pred, 0, 100)\n",
    "\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred_clipped))\n",
    "\n",
    "\n",
    "joblib.dump(model, \"ats_model.pkl\")\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
    "\n",
    "print(\"Model & vectorizer saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6480400-79b6-4d2f-993e-ebff1aa4658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(y_test, label=\"True Scores\")\n",
    "plt.plot(y_pred, label=\"Predicted Scores\")\n",
    "plt.xlabel(\"Resume Index\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"True vs Predicted Scores\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8480d8e7-0a3c-4d7a-a70b-5e076e249d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "import joblib\n",
    "\n",
    "joblib.dump(model, \"ats_model.pkl\")\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
    "\n",
    "print(\" Model and vectorizer saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc58d30-d6fc-4623-b61b-6971a2a8f670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28286fe8-2a22-4a44-8184-2d86e52fc48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import docx2txt\n",
    "from pdfminer.high_level import extract_text\n",
    "#import pdfminer.high_level\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import joblib\n",
    "from joblib import load\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "MODEL_PATH = \"ats_model.pkl\"\n",
    "VECTORIZER_PATH = \"tfidf_vectorizer.pkl\"\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')\n",
    "\n",
    "if os.path.exists(MODEL_PATH) and os.path.exists(VECTORIZER_PATH):\n",
    "    model = joblib.load(MODEL_PATH)\n",
    "    vectorizer = joblib.load(VECTORIZER_PATH)\n",
    "    MODEL_AVAILABLE = True\n",
    "else:\n",
    "    print(\"ATS model not found!\")\n",
    "    MODEL_AVAILABLE = False\n",
    "\n",
    "\n",
    "skills_df = pd.read_excel(\"skills.xlsx\")\n",
    "SKILLS_DB = skills_df.iloc[:, 0].dropna().str.lower().tolist()\n",
    "\n",
    "RESERVED_EDU_WORDS = ['university', 'college', 'institute', 'school', 'academy', 'faculty']\n",
    "\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    txt = docx2txt.process(docx_path)\n",
    "    if txt:\n",
    "        return txt.replace('\\t', ' ')\n",
    "    return \"\"\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path) or \"\"\n",
    "\n",
    "\n",
    "\n",
    "#  Extract Email & Phone\n",
    "\n",
    "def extract_contact_info(text):\n",
    "    email = None\n",
    "    phone = None\n",
    "\n",
    "    # Email\n",
    "    email_match = re.search(r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+', text)\n",
    "    if email_match:\n",
    "        email = email_match.group(0)\n",
    "\n",
    "    # Phone\n",
    "    phone_match = re.search(r'(\\+?\\d{1,3}[\\s\\-]?)?\\(?\\d{2,4}\\)?[\\s\\-]?\\d{3,5}[\\s\\-]?\\d{3,5}', text)\n",
    "    if phone_match:\n",
    "        phone = phone_match.group(0)\n",
    "\n",
    "    return email, phone\n",
    "\n",
    "\n",
    "\n",
    "#  Extract Name (first line or first proper noun pair)\n",
    "\n",
    "def extract_name(text):\n",
    "    lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "    first_line = lines[0]\n",
    "    words = nltk.word_tokenize(first_line)\n",
    "    tagged = nltk.pos_tag(words)\n",
    "\n",
    "    proper_nouns = [word for word, pos in tagged if pos == 'NNP']\n",
    "    if len(proper_nouns) >= 2:\n",
    "        return f\"{proper_nouns[0]} {proper_nouns[1]}\"\n",
    "    elif proper_nouns:\n",
    "        return proper_nouns[0]\n",
    "    return first_line\n",
    "\n",
    "#  Extract Education\n",
    "\n",
    "def extract_education(text):\n",
    "    RESERVED_EDU_WORDS = ['university', 'college', 'institute', 'school', 'academy', 'faculty']\n",
    "    DEGREE_KEYWORDS = [\n",
    "        'b.tech', 'b.e', 'bsc', 'b.s', 'bachelor',\n",
    "        'm.tech', 'm.e', 'msc', 'm.s', 'master',\n",
    "        'mba', 'phd', 'diploma', 'degree', 'b.ed'\n",
    "    ]\n",
    "\n",
    "    lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "    education_entries = set()\n",
    "\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        lower_line = lines[i].lower()\n",
    "\n",
    "        # Start only if we explicitly hit \"Education\" heading\n",
    "        if re.match(r'education', lower_line):\n",
    "            j = i + 1\n",
    "            entry = \"\"\n",
    "\n",
    "            while j < len(lines):\n",
    "                next_line = lines[j].strip()\n",
    "                # Stop if new section begins\n",
    "                if re.search(r'about\\s*me|technical skills|certifications|projects|experience|interests|extracurricular|email|phone', next_line, re.I):\n",
    "                    break\n",
    "                entry += \" \" + next_line\n",
    "                j += 1\n",
    "\n",
    "            education_entries.add(entry.strip())\n",
    "            i = j\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    return education_entries\n",
    "\n",
    "\n",
    "\n",
    "def extract_skills(text):\n",
    "\n",
    "\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    word_tokens = nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "\n",
    "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
    "\n",
    "\n",
    "    filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
    "\n",
    "\n",
    "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
    "\n",
    "\n",
    "    found_skills = set()\n",
    "\n",
    "\n",
    "    for token in filtered_tokens:\n",
    "        if token.lower() in SKILLS_DB:\n",
    "            found_skills.add(token)\n",
    "\n",
    "\n",
    "    for ngram in bigrams_trigrams:\n",
    "        if ngram.lower() in SKILLS_DB:\n",
    "            found_skills.add(ngram)\n",
    "\n",
    "    return found_skills\n",
    "\n",
    "# extract About me\n",
    "\n",
    "def extract_about_me(text):\n",
    "    lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "    about_me = \"\"\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        if re.search(r'about\\s*me', line, re.I):\n",
    "            j = i + 1\n",
    "            while j < len(lines):\n",
    "                next_line = lines[j].strip()\n",
    "                # Stop if we reach a new section\n",
    "                if re.search(r'education|technical skills|certifications|projects|experience|interests|extracurricular', next_line, re.I):\n",
    "                    break\n",
    "                about_me += \" \" + next_line\n",
    "                j += 1\n",
    "            break\n",
    "    return about_me.strip()\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    # remove punctuation and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # remove stopwords\n",
    "    stop = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop])\n",
    "\n",
    "    return text\n",
    "\n",
    "# generate a synthetic set of scores for training purpose. Act as true values\n",
    "def synthetic_score(text):\n",
    "    text = str(text).lower()\n",
    "    score = 0\n",
    "\n",
    "    # core sections\n",
    "    if \"education\" in text or \"university\" in text or \"b.tech\" in text:\n",
    "        score += 10\n",
    "    if \"experience\" in text or \"developer\" in text or \"engineer\" in text:\n",
    "        score += 10\n",
    "    if \"project\" in text:\n",
    "        score += 10\n",
    "    if \"skill\" in text or \"programming\" in text:\n",
    "        score += 10\n",
    "\n",
    "    # tech stack keywords\n",
    "    if any(k in text for k in [\"python\", \"java\", \"c++\", \"sql\", \"machine learning\", \"ai\", \"data\"]):\n",
    "        score += 10\n",
    "\n",
    "    # extras\n",
    "    if \"intern\" in text:\n",
    "        score += 5\n",
    "    if \"certificate\" in text or \"certification\" in text:\n",
    "        score += 5\n",
    "    if \"achievement\" in text or \"award\" in text:\n",
    "        score += 5\n",
    "\n",
    "    # word count (resume length)\n",
    "    wc = len(text.split())\n",
    "    if 200 < wc < 600:\n",
    "        score += 10\n",
    "    elif wc >= 600:\n",
    "        score += 5\n",
    "\n",
    "    # leadership/initiative\n",
    "    if any(k in text for k in [\"lead\", \"manage\", \"developed\", \"designed\", \"built\"]):\n",
    "        score += 10\n",
    "\n",
    "    return min(score, 100)\n",
    "\n",
    "    \n",
    "#df[\"score\"] = df[\"text\"].apply(synthetic_score)\n",
    "#print(\"Successful!\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"ðŸ“„ Enter the path to your resume (PDF or DOCX):\")\n",
    "    resume_path = input(\"> \").strip()\n",
    "\n",
    "    if resume_path.lower().endswith('.pdf'):\n",
    "        text = extract_text_from_pdf(resume_path)\n",
    "    elif resume_path.lower().endswith('.docx'):\n",
    "        text = extract_text_from_docx(resume_path)\n",
    "    else:\n",
    "        print(\"Unsupported file format. Please use PDF or DOCX.\")\n",
    "        exit()\n",
    "\n",
    "    name = extract_name(text)\n",
    "    email, phone = extract_contact_info(text)\n",
    "    skills = extract_skills(text)\n",
    "    education = extract_education(text)\n",
    "    about_me = extract_about_me(text)\n",
    "\n",
    "    print(\"\\n===============================\")\n",
    "    print(\"ðŸ“‹ Resume Summary\")\n",
    "    print(\"===============================\")\n",
    "    print(f\"Name: {name}\")\n",
    "    #print(f\"About Me: {about_me if about_me else 'Not found'}\")\n",
    "    print(f\"Email: {email}\")\n",
    "    print(f\"Phone: {phone}\")\n",
    "    print(f\"Education: {', '.join(education) if education else 'Not found'}\")\n",
    "    print(f\"Skills ({len(skills)}): {', '.join(skills)}\")\n",
    "    print(f\"About Me: {about_me if about_me else 'Not found'}\")\n",
    "    print(\"===============================\")\n",
    "\n",
    "    ats_score = synthetic_score(text)\n",
    "    print(f\"ATS score: {ats_score}/100\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90487333-0c8b-45f7-97b3-ff61f6383801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git version 2.52.0.windows.1\n"
     ]
    }
   ],
   "source": [
    "!git --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31750986-6002-41ca-9904-0e5afdeec79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global user.name \"KaustAbhinand\"\n",
    "!git config --global user.email \"kaustubhabhinan@gmail.com\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0b60605-b3e1-49b4-82d4-2cb395329de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pwd' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40541749-6732-40b4-8b58-6c0d937c5b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized empty Git repository in C:/Users/RAGUNATH/.git/\n"
     ]
    }
   ],
   "source": [
    "!git init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a6c2ec7-0c4c-4603-b482-ae29b694e116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: remote origin already exists.\n"
     ]
    }
   ],
   "source": [
    "!git remote add origin https://github.com/KaustAbhinand/resume_parser-rating.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397531e0-c3f0-4281-aa7f-378d8e8d5b83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
